{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # Работает только с английскими словами\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/timofey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/timofey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>m.kp.md</td>\n",
       "      <td>Экс-министр экономики Молдовы - главе МИДЭИ, ц...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>www.kp.by</td>\n",
       "      <td>Эта песня стала известна многим телезрителям б...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fanserials.tv</td>\n",
       "      <td>Банши 4 сезон 2 серия Бремя красоты смотреть о...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>colorbox.spb.ru</td>\n",
       "      <td>Не Беси Меня Картинки</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tula-sport.ru</td>\n",
       "      <td>В Новомосковске сыграют следж-хоккеисты алекси...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135304</th>\n",
       "      <td>135304</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>пора тюльпанов турецкий сериал на русском язык...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135305</th>\n",
       "      <td>135305</td>\n",
       "      <td>www.ntv.ru</td>\n",
       "      <td>Остросюжетный сериал «Шеф. Игра на повышение»....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135306</th>\n",
       "      <td>135306</td>\n",
       "      <td>topclassiccarsforsale.com</td>\n",
       "      <td>1941 Plymouth Special Deluxe Hot Rod, Automati...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135307</th>\n",
       "      <td>135307</td>\n",
       "      <td>wowcream.ru</td>\n",
       "      <td>Купить It's Skin Сыворотка питательная Power 1...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135308</th>\n",
       "      <td>135308</td>\n",
       "      <td>www.ubu.ru</td>\n",
       "      <td>Технический спирт в канистрах и флаконах, купи...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                        url  \\\n",
       "0            0                    m.kp.md   \n",
       "1            1                  www.kp.by   \n",
       "2            2              fanserials.tv   \n",
       "3            3            colorbox.spb.ru   \n",
       "4            4              tula-sport.ru   \n",
       "...        ...                        ...   \n",
       "135304  135304                    mail.ru   \n",
       "135305  135305                 www.ntv.ru   \n",
       "135306  135306  topclassiccarsforsale.com   \n",
       "135307  135307                wowcream.ru   \n",
       "135308  135308                 www.ubu.ru   \n",
       "\n",
       "                                                    title  target  \n",
       "0       Экс-министр экономики Молдовы - главе МИДЭИ, ц...   False  \n",
       "1       Эта песня стала известна многим телезрителям б...   False  \n",
       "2       Банши 4 сезон 2 серия Бремя красоты смотреть о...   False  \n",
       "3                                   Не Беси Меня Картинки   False  \n",
       "4       В Новомосковске сыграют следж-хоккеисты алекси...   False  \n",
       "...                                                   ...     ...  \n",
       "135304  пора тюльпанов турецкий сериал на русском язык...   False  \n",
       "135305  Остросюжетный сериал «Шеф. Игра на повышение»....   False  \n",
       "135306  1941 Plymouth Special Deluxe Hot Rod, Automati...   False  \n",
       "135307  Купить It's Skin Сыворотка питательная Power 1...   False  \n",
       "135308  Технический спирт в канистрах и флаконах, купи...   False  \n",
       "\n",
       "[135309 rows x 4 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бегущие'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"бегущие\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежать'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "lemmatizer.parse(\"бегущие\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежать'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "''.join(m.lemmatize(\"бегущие\")).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "# stop_words = set(stopwords.words('russian')) | set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(data):\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data['title'] = data['title'].apply(lambda x: x.lower())\n",
    "    # Удаляем все, кроме слов и цифр с пробелами\n",
    "    data['title'] = data['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) \n",
    "    data['title'] = data['title'].apply(lambda x: re.sub(r'\\d+', '', x)) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разные лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordNetLemmatize(train_data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return train_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MystemLemmatize(train_data):\n",
    "    # Инициализация объекта для лемматизации\n",
    "    mystem = Mystem()\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Лемматизация\n",
    "        lemmas = mystem.lemmatize(text)\n",
    "        # Удаление стоп-слов\n",
    "        lemmas = [lemma.strip() for lemma in lemmas if lemma not in stop_words]\n",
    "        # Склеивание лемм в одну строку\n",
    "        text = ' '.join(lemmas)\n",
    "        return text\n",
    "\n",
    "    return train_data.apply(lambda text: preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy2Lemmatize(train_data):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    return train_data.apply(\n",
    "        lambda x: ' '.join([lemmatizer.parse(word)[0].normal_form for word in x.split() if word not in stop_words])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title'] = pymorphy2Lemmatize(train_data['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование текста в числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['text'] = train_data['title'].apply(lambda x: x + ' ') + train_data['url']\n",
    "# Преобразование текста в числа\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_data['title'])\n",
    "y = train_data['target']\n",
    "\n",
    "# Разбиение данных на обучающую и валидационную выборки\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['эксминистр' 'экономика' 'молдова' ... 'blockpower' 'effector' 'wowcream']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Презентация на тему \"Приближенное значение. Абсолютная и относительная погрешнос\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['гидромассажный',\n",
       " 'olgafisakovamailru',\n",
       " 'юнитекс',\n",
       " 'размешать',\n",
       " 'alvares',\n",
       " 'grou',\n",
       " 'кренк']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = 42\n",
    "\n",
    "print(X_train[id_])\n",
    "\n",
    "x_vector = X.getrow(id_).toarray()[0]\n",
    "\n",
    "[feature for feature in feature_names[x_vector > 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества модели на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.989\n",
      "F1-score на валидационной выборке: 0.969\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "score = f1_score(y_train, y_pred)\n",
    "print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_val)\n",
    "print(\"F1-score на валидационной выборке: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединяем обработки текста в отдельную функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, with_report = False,\n",
    "                model=LogisticRegression(),\n",
    "                Lemmatizer = None, \n",
    "                stop_words = set(stopwords.words('russian')),\n",
    "               ):\n",
    "    # Очищаем\n",
    "    train_data = clean_data(train_data)\n",
    "    # Лемматизируем\n",
    "    if (Lemmatizer is not None):\n",
    "        train_data['title'] = Lemmatizer(train_data['title'])\n",
    "    # Приводим к числам\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(train_data['title'])\n",
    "    y = train_data['target']\n",
    "    \n",
    "    # Разбиение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if (with_report):\n",
    "        y_pred = model.predict(X_train)\n",
    "        score = f1_score(y_train, y_pred)\n",
    "        print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        score = f1_score(y_val, y_pred_val)\n",
    "        print(\"F1-score на валидационной выборке: {:.3f}\".format(score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_pymorphy2Lemmatize = train_model(train_data, with_report=True, Lemmatizer=pymorphy2Lemmatize, stop_words=stop_words_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_pymorphy2Lemmatize = train_model(train_data, with_report=True, Lemmatizer=pymorphy2Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_MystemLemmatize = train_model(train_data, with_report=True, Lemmatizer=MystemLemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.939\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "model_WordNetLemmatize = train_model(train_data, with_report=True, Lemmatizer=WordNetLemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.939\n"
     ]
    }
   ],
   "source": [
    "model_without_lemmatize = train_model(train_data, with_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем трансформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Инициализация токенизатора и модели\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Обработка текста и получение эмбеддингов\n",
    "text = \"This is a sample text to be embedded.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Инициализация токенизатора и модели\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 / 135309 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Обработка текста и получение эмбеддингов\n",
    "i = 0\n",
    "X = []\n",
    "for text in train_data['title']:\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    X.append(embeddings.detach().numpy())\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Iteration {i} /\",train_data['title'].shape[0], f\"{i/train_data['title'].shape[0]}%\")\n",
    "    i+=1\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Чтение из файла, если файл существует\n",
    "if os.path.isfile('embeddings.csv'):\n",
    "    X = pd.read_csv('embeddings.csv', index_col=0)\n",
    "else:\n",
    "    X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20 / 135309 (0.01%)\n",
      "Iteration 30 / 135309 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "start_idx = X.shape[0]\n",
    "# Обработка текста и получение эмбеддингов\n",
    "for i, text in enumerate(train_data['title'][start_idx:start_idx+20]):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "#     X = pd.DataFrame(embeddings.detach().numpy()).T\n",
    "    X.loc[i] = embeddings.detach().numpy()\n",
    "    if (i % 10 == 0):\n",
    "        print(f\"Iteration {i+start_idx} / {train_data['title'].shape[0]} ({(i+start_idx)/train_data['title'].shape[0]*100:.2f}%)\")\n",
    "        # Запись в файл на каждой 1000-й итерации\n",
    "        X.to_csv('embeddings.csv')        \n",
    "# Запись оставшихся данных в файл\n",
    "X.to_csv('embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130314</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.108823</td>\n",
       "      <td>-0.040220</td>\n",
       "      <td>0.579633</td>\n",
       "      <td>0.147070</td>\n",
       "      <td>-0.058492</td>\n",
       "      <td>0.342769</td>\n",
       "      <td>-0.002201</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040137</td>\n",
       "      <td>-0.315638</td>\n",
       "      <td>0.343903</td>\n",
       "      <td>-0.471850</td>\n",
       "      <td>0.218975</td>\n",
       "      <td>-0.796071</td>\n",
       "      <td>-0.057456</td>\n",
       "      <td>-0.188640</td>\n",
       "      <td>0.610182</td>\n",
       "      <td>0.159787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.077997</td>\n",
       "      <td>-0.103543</td>\n",
       "      <td>0.090305</td>\n",
       "      <td>0.031430</td>\n",
       "      <td>0.561107</td>\n",
       "      <td>0.140605</td>\n",
       "      <td>-0.112423</td>\n",
       "      <td>0.485146</td>\n",
       "      <td>-0.077454</td>\n",
       "      <td>-0.183468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094906</td>\n",
       "      <td>-0.461445</td>\n",
       "      <td>0.184997</td>\n",
       "      <td>-0.397397</td>\n",
       "      <td>0.094669</td>\n",
       "      <td>-0.684273</td>\n",
       "      <td>0.087932</td>\n",
       "      <td>-0.117875</td>\n",
       "      <td>0.593566</td>\n",
       "      <td>0.211949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.113480</td>\n",
       "      <td>-0.040033</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>0.671913</td>\n",
       "      <td>0.149452</td>\n",
       "      <td>-0.187717</td>\n",
       "      <td>0.498494</td>\n",
       "      <td>-0.027705</td>\n",
       "      <td>-0.027285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041938</td>\n",
       "      <td>-0.346197</td>\n",
       "      <td>0.439110</td>\n",
       "      <td>-0.334250</td>\n",
       "      <td>0.187062</td>\n",
       "      <td>-0.893234</td>\n",
       "      <td>-0.104641</td>\n",
       "      <td>-0.212868</td>\n",
       "      <td>0.586935</td>\n",
       "      <td>0.105043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.056768</td>\n",
       "      <td>0.088331</td>\n",
       "      <td>-0.115944</td>\n",
       "      <td>-0.165607</td>\n",
       "      <td>0.203022</td>\n",
       "      <td>-0.086711</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.257392</td>\n",
       "      <td>-0.011821</td>\n",
       "      <td>-0.039709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034242</td>\n",
       "      <td>-0.129220</td>\n",
       "      <td>-0.052545</td>\n",
       "      <td>-0.281699</td>\n",
       "      <td>0.073281</td>\n",
       "      <td>-0.232753</td>\n",
       "      <td>-0.129449</td>\n",
       "      <td>-0.238482</td>\n",
       "      <td>0.194678</td>\n",
       "      <td>0.286459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.117689</td>\n",
       "      <td>-0.098321</td>\n",
       "      <td>0.058717</td>\n",
       "      <td>-0.034988</td>\n",
       "      <td>0.526780</td>\n",
       "      <td>0.208661</td>\n",
       "      <td>-0.046965</td>\n",
       "      <td>0.394215</td>\n",
       "      <td>-0.022430</td>\n",
       "      <td>-0.163613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053588</td>\n",
       "      <td>-0.463544</td>\n",
       "      <td>0.278821</td>\n",
       "      <td>-0.391382</td>\n",
       "      <td>0.173528</td>\n",
       "      <td>-0.838697</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>-0.146310</td>\n",
       "      <td>0.666737</td>\n",
       "      <td>0.055644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.069972</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>-0.085899</td>\n",
       "      <td>0.574031</td>\n",
       "      <td>0.206851</td>\n",
       "      <td>0.043749</td>\n",
       "      <td>0.470226</td>\n",
       "      <td>0.124416</td>\n",
       "      <td>-0.155512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092336</td>\n",
       "      <td>-0.307095</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>-0.240241</td>\n",
       "      <td>0.088650</td>\n",
       "      <td>-0.902267</td>\n",
       "      <td>-0.145190</td>\n",
       "      <td>-0.374592</td>\n",
       "      <td>0.426418</td>\n",
       "      <td>-0.005960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.152178</td>\n",
       "      <td>-0.152408</td>\n",
       "      <td>-0.098346</td>\n",
       "      <td>-0.037491</td>\n",
       "      <td>0.598458</td>\n",
       "      <td>0.094544</td>\n",
       "      <td>-0.116980</td>\n",
       "      <td>0.492469</td>\n",
       "      <td>-0.048189</td>\n",
       "      <td>-0.228956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027289</td>\n",
       "      <td>-0.398486</td>\n",
       "      <td>0.319981</td>\n",
       "      <td>-0.416669</td>\n",
       "      <td>0.174291</td>\n",
       "      <td>-0.646413</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>-0.254155</td>\n",
       "      <td>0.449767</td>\n",
       "      <td>-0.030907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.072970</td>\n",
       "      <td>-0.154533</td>\n",
       "      <td>-0.037140</td>\n",
       "      <td>-0.067041</td>\n",
       "      <td>0.580667</td>\n",
       "      <td>0.242237</td>\n",
       "      <td>-0.083023</td>\n",
       "      <td>0.591747</td>\n",
       "      <td>-0.061652</td>\n",
       "      <td>-0.092554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084779</td>\n",
       "      <td>-0.512956</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>-0.356376</td>\n",
       "      <td>0.157623</td>\n",
       "      <td>-1.023206</td>\n",
       "      <td>-0.009973</td>\n",
       "      <td>-0.232336</td>\n",
       "      <td>0.558031</td>\n",
       "      <td>0.019852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.091318</td>\n",
       "      <td>-0.079259</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>-0.021458</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.209741</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>0.541909</td>\n",
       "      <td>-0.068774</td>\n",
       "      <td>-0.126115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020999</td>\n",
       "      <td>-0.515429</td>\n",
       "      <td>0.286397</td>\n",
       "      <td>-0.423510</td>\n",
       "      <td>0.096842</td>\n",
       "      <td>-0.875140</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>-0.235448</td>\n",
       "      <td>0.606358</td>\n",
       "      <td>0.057077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.170166</td>\n",
       "      <td>0.054747</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>-0.050119</td>\n",
       "      <td>0.495358</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>-0.025543</td>\n",
       "      <td>0.257664</td>\n",
       "      <td>0.093114</td>\n",
       "      <td>-0.049816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131567</td>\n",
       "      <td>-0.283757</td>\n",
       "      <td>0.450232</td>\n",
       "      <td>-0.444589</td>\n",
       "      <td>0.209589</td>\n",
       "      <td>-1.037264</td>\n",
       "      <td>-0.196315</td>\n",
       "      <td>-0.211210</td>\n",
       "      <td>0.611195</td>\n",
       "      <td>0.081018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.092984</td>\n",
       "      <td>-0.016568</td>\n",
       "      <td>0.044869</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.399416</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>-0.033510</td>\n",
       "      <td>0.378868</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>-0.043390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123133</td>\n",
       "      <td>-0.490159</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>-0.321781</td>\n",
       "      <td>-0.029329</td>\n",
       "      <td>-0.836296</td>\n",
       "      <td>-0.012359</td>\n",
       "      <td>-0.135115</td>\n",
       "      <td>0.525350</td>\n",
       "      <td>0.013168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.014294</td>\n",
       "      <td>-0.100831</td>\n",
       "      <td>0.161361</td>\n",
       "      <td>-0.085731</td>\n",
       "      <td>0.566456</td>\n",
       "      <td>0.136563</td>\n",
       "      <td>-0.117053</td>\n",
       "      <td>0.338369</td>\n",
       "      <td>-0.102560</td>\n",
       "      <td>-0.005683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>-0.284058</td>\n",
       "      <td>0.287726</td>\n",
       "      <td>-0.479005</td>\n",
       "      <td>0.240835</td>\n",
       "      <td>-0.737150</td>\n",
       "      <td>-0.077228</td>\n",
       "      <td>-0.167112</td>\n",
       "      <td>0.556710</td>\n",
       "      <td>0.098414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.087482</td>\n",
       "      <td>-0.171771</td>\n",
       "      <td>-0.205437</td>\n",
       "      <td>-0.117878</td>\n",
       "      <td>0.578208</td>\n",
       "      <td>-0.020864</td>\n",
       "      <td>-0.057323</td>\n",
       "      <td>0.406736</td>\n",
       "      <td>-0.274169</td>\n",
       "      <td>-0.226006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050931</td>\n",
       "      <td>-0.386013</td>\n",
       "      <td>0.298592</td>\n",
       "      <td>-0.310632</td>\n",
       "      <td>0.205810</td>\n",
       "      <td>-0.644255</td>\n",
       "      <td>0.021055</td>\n",
       "      <td>-0.297254</td>\n",
       "      <td>0.371845</td>\n",
       "      <td>-0.007038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.077087</td>\n",
       "      <td>0.062671</td>\n",
       "      <td>0.190402</td>\n",
       "      <td>-0.020414</td>\n",
       "      <td>0.552633</td>\n",
       "      <td>0.085839</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.240642</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>-0.020208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084035</td>\n",
       "      <td>-0.331091</td>\n",
       "      <td>0.396108</td>\n",
       "      <td>-0.416581</td>\n",
       "      <td>0.245136</td>\n",
       "      <td>-0.870213</td>\n",
       "      <td>-0.159243</td>\n",
       "      <td>-0.181244</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.130627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.248449</td>\n",
       "      <td>-0.224073</td>\n",
       "      <td>-0.057077</td>\n",
       "      <td>-0.105248</td>\n",
       "      <td>0.123019</td>\n",
       "      <td>-0.069706</td>\n",
       "      <td>0.142135</td>\n",
       "      <td>-0.087245</td>\n",
       "      <td>0.175768</td>\n",
       "      <td>-0.105299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073567</td>\n",
       "      <td>0.156897</td>\n",
       "      <td>-0.015277</td>\n",
       "      <td>-0.083945</td>\n",
       "      <td>0.225581</td>\n",
       "      <td>-0.170564</td>\n",
       "      <td>-0.036863</td>\n",
       "      <td>-0.250007</td>\n",
       "      <td>0.038070</td>\n",
       "      <td>-0.026419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.008544</td>\n",
       "      <td>-0.155952</td>\n",
       "      <td>-0.022414</td>\n",
       "      <td>-0.082535</td>\n",
       "      <td>0.619016</td>\n",
       "      <td>0.201334</td>\n",
       "      <td>-0.118269</td>\n",
       "      <td>0.291741</td>\n",
       "      <td>-0.074017</td>\n",
       "      <td>-0.090908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055050</td>\n",
       "      <td>-0.328956</td>\n",
       "      <td>0.266470</td>\n",
       "      <td>-0.422724</td>\n",
       "      <td>0.284303</td>\n",
       "      <td>-0.790091</td>\n",
       "      <td>-0.154315</td>\n",
       "      <td>-0.232341</td>\n",
       "      <td>0.518740</td>\n",
       "      <td>-0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.065810</td>\n",
       "      <td>-0.035854</td>\n",
       "      <td>-0.098968</td>\n",
       "      <td>-0.092348</td>\n",
       "      <td>0.612902</td>\n",
       "      <td>0.221904</td>\n",
       "      <td>0.027452</td>\n",
       "      <td>0.314216</td>\n",
       "      <td>-0.057063</td>\n",
       "      <td>-0.042258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>-0.366139</td>\n",
       "      <td>0.322027</td>\n",
       "      <td>-0.401552</td>\n",
       "      <td>0.182097</td>\n",
       "      <td>-0.792380</td>\n",
       "      <td>-0.122874</td>\n",
       "      <td>-0.155467</td>\n",
       "      <td>0.535316</td>\n",
       "      <td>0.075168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.161209</td>\n",
       "      <td>0.086351</td>\n",
       "      <td>0.259944</td>\n",
       "      <td>-0.084423</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>-0.062394</td>\n",
       "      <td>0.280887</td>\n",
       "      <td>0.233109</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042736</td>\n",
       "      <td>-0.312950</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>-0.260120</td>\n",
       "      <td>-0.047339</td>\n",
       "      <td>-0.433464</td>\n",
       "      <td>0.044718</td>\n",
       "      <td>-0.022887</td>\n",
       "      <td>0.299137</td>\n",
       "      <td>0.095899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.115386</td>\n",
       "      <td>-0.055757</td>\n",
       "      <td>0.161556</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.215508</td>\n",
       "      <td>-0.149828</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.160282</td>\n",
       "      <td>-0.049787</td>\n",
       "      <td>-0.248023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131438</td>\n",
       "      <td>0.091465</td>\n",
       "      <td>0.130347</td>\n",
       "      <td>-0.348610</td>\n",
       "      <td>0.036559</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>-0.148535</td>\n",
       "      <td>0.109994</td>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.019474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.057146</td>\n",
       "      <td>0.117013</td>\n",
       "      <td>-0.075902</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>0.202099</td>\n",
       "      <td>-0.111476</td>\n",
       "      <td>-0.051106</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>-0.052392</td>\n",
       "      <td>-0.288108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192896</td>\n",
       "      <td>-0.175556</td>\n",
       "      <td>0.134141</td>\n",
       "      <td>-0.300285</td>\n",
       "      <td>0.067129</td>\n",
       "      <td>-0.127411</td>\n",
       "      <td>-0.183086</td>\n",
       "      <td>0.181553</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>0.113783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.130314  0.005657  0.108823 -0.040220  0.579633  0.147070 -0.058492   \n",
       "1  -0.077997 -0.103543  0.090305  0.031430  0.561107  0.140605 -0.112423   \n",
       "2  -0.113480 -0.040033  0.061344  0.031506  0.671913  0.149452 -0.187717   \n",
       "3  -0.056768  0.088331 -0.115944 -0.165607  0.203022 -0.086711  0.010639   \n",
       "4  -0.117689 -0.098321  0.058717 -0.034988  0.526780  0.208661 -0.046965   \n",
       "5  -0.069972  0.002838  0.060972 -0.085899  0.574031  0.206851  0.043749   \n",
       "6  -0.152178 -0.152408 -0.098346 -0.037491  0.598458  0.094544 -0.116980   \n",
       "7   0.072970 -0.154533 -0.037140 -0.067041  0.580667  0.242237 -0.083023   \n",
       "8  -0.091318 -0.079259  0.030702 -0.021458  0.611106  0.209741 -0.001717   \n",
       "9  -0.170166  0.054747  0.195478 -0.050119  0.495358  0.064851 -0.025543   \n",
       "10 -0.092984 -0.016568  0.044869  0.070599  0.399416  0.229492 -0.033510   \n",
       "11 -0.014294 -0.100831  0.161361 -0.085731  0.566456  0.136563 -0.117053   \n",
       "12  0.087482 -0.171771 -0.205437 -0.117878  0.578208 -0.020864 -0.057323   \n",
       "13 -0.077087  0.062671  0.190402 -0.020414  0.552633  0.085839  0.002980   \n",
       "14  0.248449 -0.224073 -0.057077 -0.105248  0.123019 -0.069706  0.142135   \n",
       "15 -0.008544 -0.155952 -0.022414 -0.082535  0.619016  0.201334 -0.118269   \n",
       "16 -0.065810 -0.035854 -0.098968 -0.092348  0.612902  0.221904  0.027452   \n",
       "17  0.161209  0.086351  0.259944 -0.084423  0.294800 -0.062394  0.280887   \n",
       "18  0.115386 -0.055757  0.161556  0.002562  0.215508 -0.149828 -0.013604   \n",
       "19  0.057146  0.117013 -0.075902 -0.004653  0.202099 -0.111476 -0.051106   \n",
       "\n",
       "         7         8         9    ...       758       759       760       761  \\\n",
       "0   0.342769 -0.002201 -0.022413  ...  0.040137 -0.315638  0.343903 -0.471850   \n",
       "1   0.485146 -0.077454 -0.183468  ...  0.094906 -0.461445  0.184997 -0.397397   \n",
       "2   0.498494 -0.027705 -0.027285  ... -0.041938 -0.346197  0.439110 -0.334250   \n",
       "3   0.257392 -0.011821 -0.039709  ... -0.034242 -0.129220 -0.052545 -0.281699   \n",
       "4   0.394215 -0.022430 -0.163613  ...  0.053588 -0.463544  0.278821 -0.391382   \n",
       "5   0.470226  0.124416 -0.155512  ... -0.092336 -0.307095  0.250008 -0.240241   \n",
       "6   0.492469 -0.048189 -0.228956  ...  0.027289 -0.398486  0.319981 -0.416669   \n",
       "7   0.591747 -0.061652 -0.092554  ... -0.084779 -0.512956  0.344828 -0.356376   \n",
       "8   0.541909 -0.068774 -0.126115  ... -0.020999 -0.515429  0.286397 -0.423510   \n",
       "9   0.257664  0.093114 -0.049816  ... -0.131567 -0.283757  0.450232 -0.444589   \n",
       "10  0.378868  0.014931 -0.043390  ... -0.123133 -0.490159  0.188227 -0.321781   \n",
       "11  0.338369 -0.102560 -0.005683  ...  0.012482 -0.284058  0.287726 -0.479005   \n",
       "12  0.406736 -0.274169 -0.226006  ...  0.050931 -0.386013  0.298592 -0.310632   \n",
       "13  0.240642  0.024127 -0.020208  ... -0.084035 -0.331091  0.396108 -0.416581   \n",
       "14 -0.087245  0.175768 -0.105299  ...  0.073567  0.156897 -0.015277 -0.083945   \n",
       "15  0.291741 -0.074017 -0.090908  ... -0.055050 -0.328956  0.266470 -0.422724   \n",
       "16  0.314216 -0.057063 -0.042258  ... -0.046894 -0.366139  0.322027 -0.401552   \n",
       "17  0.233109  0.009784 -0.015722  ... -0.042736 -0.312950  0.067810 -0.260120   \n",
       "18  0.160282 -0.049787 -0.248023  ...  0.131438  0.091465  0.130347 -0.348610   \n",
       "19  0.050217 -0.052392 -0.288108  ...  0.192896 -0.175556  0.134141 -0.300285   \n",
       "\n",
       "         762       763       764       765       766       767  \n",
       "0   0.218975 -0.796071 -0.057456 -0.188640  0.610182  0.159787  \n",
       "1   0.094669 -0.684273  0.087932 -0.117875  0.593566  0.211949  \n",
       "2   0.187062 -0.893234 -0.104641 -0.212868  0.586935  0.105043  \n",
       "3   0.073281 -0.232753 -0.129449 -0.238482  0.194678  0.286459  \n",
       "4   0.173528 -0.838697  0.014650 -0.146310  0.666737  0.055644  \n",
       "5   0.088650 -0.902267 -0.145190 -0.374592  0.426418 -0.005960  \n",
       "6   0.174291 -0.646413  0.021024 -0.254155  0.449767 -0.030907  \n",
       "7   0.157623 -1.023206 -0.009973 -0.232336  0.558031  0.019852  \n",
       "8   0.096842 -0.875140  0.036671 -0.235448  0.606358  0.057077  \n",
       "9   0.209589 -1.037264 -0.196315 -0.211210  0.611195  0.081018  \n",
       "10 -0.029329 -0.836296 -0.012359 -0.135115  0.525350  0.013168  \n",
       "11  0.240835 -0.737150 -0.077228 -0.167112  0.556710  0.098414  \n",
       "12  0.205810 -0.644255  0.021055 -0.297254  0.371845 -0.007038  \n",
       "13  0.245136 -0.870213 -0.159243 -0.181244  0.577350  0.130627  \n",
       "14  0.225581 -0.170564 -0.036863 -0.250007  0.038070 -0.026419  \n",
       "15  0.284303 -0.790091 -0.154315 -0.232341  0.518740 -0.002488  \n",
       "16  0.182097 -0.792380 -0.122874 -0.155467  0.535316  0.075168  \n",
       "17 -0.047339 -0.433464  0.044718 -0.022887  0.299137  0.095899  \n",
       "18  0.036559 -0.070479 -0.148535  0.109994  0.008113  0.019474  \n",
       "19  0.067129 -0.127411 -0.183086  0.181553  0.018564  0.113783  \n",
       "\n",
       "[20 rows x 768 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тренировочной выборке: 0.7992678462477121\n",
      "F1-score на валидационной выборке: 0.6945812807881773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timofey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "y = train_data['target'][:10000]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка качества модели\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "print('F1-score на тренировочной выборке:', f1_train)\n",
    "print('F1-score на валидационной выборке:', f1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False    148569\n",
    "True      16809\n",
    "Удаление повторных слов:\n",
    "False    148499\n",
    "True      16879\n",
    "Добавление url в список слов: при этом keggle стал хуже\n",
    "False    148878\n",
    "True      16500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    148878\n",
      "True      16500\n",
      "Name: target, dtype: int64\n",
      "id,target\r\n",
      "135309,False\r\n",
      "135310,False\r\n",
      "135311,False\r\n",
      "135312,True\r\n",
      "135313,False\r\n",
      "135314,False\r\n",
      "135315,False\r\n",
      "135316,False\r\n",
      "135317,False\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# X_test_vectorized = vectorizer.transform(test_df[\"title\"].values)\n",
    "X_test_vectorized = vectorizer.transform(test_df[\"title\"])\n",
    "\n",
    "test_df[\"target\"] = model.predict(X_test_vectorized).astype(bool)\n",
    "\n",
    "counts = test_df[\"target\"].value_counts()\n",
    "print(counts)\n",
    "\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"ml_baseline.csv\", index=False)\n",
    "\n",
    "!cat ml_baseline.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
