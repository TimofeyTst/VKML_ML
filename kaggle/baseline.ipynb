{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # Работает только с английскими словами\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/timofey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/timofey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>m.kp.md</td>\n",
       "      <td>Экс-министр экономики Молдовы - главе МИДЭИ, ц...</td>\n",
       "      <td>False</td>\n",
       "      <td>Экс-министр экономики Молдовы - главе МИДЭИ, ц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>www.kp.by</td>\n",
       "      <td>Эта песня стала известна многим телезрителям б...</td>\n",
       "      <td>False</td>\n",
       "      <td>Эта песня стала известна многим телезрителям б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fanserials.tv</td>\n",
       "      <td>Банши 4 сезон 2 серия Бремя красоты смотреть о...</td>\n",
       "      <td>False</td>\n",
       "      <td>Банши 4 сезон 2 серия Бремя красоты смотреть о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>colorbox.spb.ru</td>\n",
       "      <td>Не Беси Меня Картинки</td>\n",
       "      <td>False</td>\n",
       "      <td>Не Беси Меня Картинки colorbox.spb.ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tula-sport.ru</td>\n",
       "      <td>В Новомосковске сыграют следж-хоккеисты алекси...</td>\n",
       "      <td>False</td>\n",
       "      <td>В Новомосковске сыграют следж-хоккеисты алекси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135304</th>\n",
       "      <td>135304</td>\n",
       "      <td>mail.ru</td>\n",
       "      <td>пора тюльпанов турецкий сериал на русском язык...</td>\n",
       "      <td>False</td>\n",
       "      <td>пора тюльпанов турецкий сериал на русском язык...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135305</th>\n",
       "      <td>135305</td>\n",
       "      <td>www.ntv.ru</td>\n",
       "      <td>Остросюжетный сериал «Шеф. Игра на повышение»....</td>\n",
       "      <td>False</td>\n",
       "      <td>Остросюжетный сериал «Шеф. Игра на повышение»....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135306</th>\n",
       "      <td>135306</td>\n",
       "      <td>topclassiccarsforsale.com</td>\n",
       "      <td>1941 Plymouth Special Deluxe Hot Rod, Automati...</td>\n",
       "      <td>False</td>\n",
       "      <td>1941 Plymouth Special Deluxe Hot Rod, Automati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135307</th>\n",
       "      <td>135307</td>\n",
       "      <td>wowcream.ru</td>\n",
       "      <td>Купить It's Skin Сыворотка питательная Power 1...</td>\n",
       "      <td>False</td>\n",
       "      <td>Купить It's Skin Сыворотка питательная Power 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135308</th>\n",
       "      <td>135308</td>\n",
       "      <td>www.ubu.ru</td>\n",
       "      <td>Технический спирт в канистрах и флаконах, купи...</td>\n",
       "      <td>False</td>\n",
       "      <td>Технический спирт в канистрах и флаконах, купи...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135309 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                        url  \\\n",
       "0            0                    m.kp.md   \n",
       "1            1                  www.kp.by   \n",
       "2            2              fanserials.tv   \n",
       "3            3            colorbox.spb.ru   \n",
       "4            4              tula-sport.ru   \n",
       "...        ...                        ...   \n",
       "135304  135304                    mail.ru   \n",
       "135305  135305                 www.ntv.ru   \n",
       "135306  135306  topclassiccarsforsale.com   \n",
       "135307  135307                wowcream.ru   \n",
       "135308  135308                 www.ubu.ru   \n",
       "\n",
       "                                                    title  target  \\\n",
       "0       Экс-министр экономики Молдовы - главе МИДЭИ, ц...   False   \n",
       "1       Эта песня стала известна многим телезрителям б...   False   \n",
       "2       Банши 4 сезон 2 серия Бремя красоты смотреть о...   False   \n",
       "3                                   Не Беси Меня Картинки   False   \n",
       "4       В Новомосковске сыграют следж-хоккеисты алекси...   False   \n",
       "...                                                   ...     ...   \n",
       "135304  пора тюльпанов турецкий сериал на русском язык...   False   \n",
       "135305  Остросюжетный сериал «Шеф. Игра на повышение»....   False   \n",
       "135306  1941 Plymouth Special Deluxe Hot Rod, Automati...   False   \n",
       "135307  Купить It's Skin Сыворотка питательная Power 1...   False   \n",
       "135308  Технический спирт в канистрах и флаконах, купи...   False   \n",
       "\n",
       "                                                     text  \n",
       "0       Экс-министр экономики Молдовы - главе МИДЭИ, ц...  \n",
       "1       Эта песня стала известна многим телезрителям б...  \n",
       "2       Банши 4 сезон 2 серия Бремя красоты смотреть о...  \n",
       "3                   Не Беси Меня Картинки colorbox.spb.ru  \n",
       "4       В Новомосковске сыграют следж-хоккеисты алекси...  \n",
       "...                                                   ...  \n",
       "135304  пора тюльпанов турецкий сериал на русском язык...  \n",
       "135305  Остросюжетный сериал «Шеф. Игра на повышение»....  \n",
       "135306  1941 Plymouth Special Deluxe Hot Rod, Automati...  \n",
       "135307  Купить It's Skin Сыворотка питательная Power 1...  \n",
       "135308  Технический спирт в канистрах и флаконах, купи...  \n",
       "\n",
       "[135309 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бегущие'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"бегущие\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежать'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "lemmatizer.parse(\"бегущие\")[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежать'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "''.join(m.lemmatize(\"бегущие\")).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "# stop_words = set(stopwords.words('russian')) | set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(data):\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data['title'] = data['title'].apply(lambda x: x.lower())\n",
    "    # Удаляем все, кроме слов и цифр с пробелами\n",
    "    data['title'] = data['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) \n",
    "    data['title'] = data['title'].apply(lambda x: re.sub(r'\\d+', '', x)) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разные лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordNetLemmatize(train_data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return train_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MystemLemmatize(train_data):\n",
    "    # Инициализация объекта для лемматизации\n",
    "    mystem = Mystem()\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Лемматизация\n",
    "        lemmas = mystem.lemmatize(text)\n",
    "        # Удаление стоп-слов\n",
    "        lemmas = [lemma.strip() for lemma in lemmas if lemma not in stop_words]\n",
    "        # Склеивание лемм в одну строку\n",
    "        text = ' '.join(lemmas)\n",
    "        return text\n",
    "\n",
    "    return train_data.apply(lambda text: preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy2Lemmatize(train_data):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    return train_data.apply(\n",
    "        lambda x: ' '.join([lemmatizer.parse(word)[0].normal_form for word in x.split() if word not in stop_words])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title'] = MystemLemmatize(train_data['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование текста в числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_data['text'] = train_data['title'].apply(lambda x: x + ' ') + train_data['url']\n",
    "# Преобразование текста в числа\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(train_data['text'])\n",
    "y = train_data['target']\n",
    "\n",
    "# Разбиение данных на обучающую и валидационную выборки\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['эксминистр' 'экономика' 'молдова' ... 'blockpower' 'effector' 'wowcream']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Презентация на тему \"Приближенное значение. Абсолютная и относительная погрешнос\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['гидромассажный',\n",
       " 'olgafisakovamailru',\n",
       " 'юнитекс',\n",
       " 'размешать',\n",
       " 'alvares',\n",
       " 'grou',\n",
       " 'кренк']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = 42\n",
    "\n",
    "print(X_train[id_])\n",
    "\n",
    "x_vector = X.getrow(id_).toarray()[0]\n",
    "\n",
    "[feature for feature in feature_names[x_vector > 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timofey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества модели на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.989\n",
      "F1-score на валидационной выборке: 0.969\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "score = f1_score(y_train, y_pred)\n",
    "print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_val)\n",
    "print(\"F1-score на валидационной выборке: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.990\n",
      "F1-score на валидационной выборке: 0.967\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "score = f1_score(y_train, y_pred)\n",
    "print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_val)\n",
    "print(\"F1-score на валидационной выборке: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.989\n",
      "F1-score на валидационной выборке: 0.989\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "score = f1_score(y_train, y_pred)\n",
    "print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_val)\n",
    "print(\"F1-score на валидационной выборке: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединяем обработки текста в отдельную функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, with_report = False,\n",
    "                model=LogisticRegression(),\n",
    "                Lemmatizer = None, \n",
    "                stop_words = set(stopwords.words('russian')),\n",
    "               ):\n",
    "    # Очищаем\n",
    "    train_data = clean_data(train_data)\n",
    "    # Лемматизируем\n",
    "    if (Lemmatizer is not None):\n",
    "        train_data['title'] = Lemmatizer(train_data['title'])\n",
    "    # Приводим к числам\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(train_data['title'])\n",
    "    y = train_data['target']\n",
    "    \n",
    "    # Разбиение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if (with_report):\n",
    "        y_pred = model.predict(X_train)\n",
    "        score = f1_score(y_train, y_pred)\n",
    "        print(\"F1-score на тестовой выборке: {:.3f}\".format(score))\n",
    "\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        score = f1_score(y_val, y_pred_val)\n",
    "        print(\"F1-score на валидационной выборке: {:.3f}\".format(score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_pymorphy2Lemmatize = train_model(train_data, with_report=True, Lemmatizer=pymorphy2Lemmatize, stop_words=stop_words_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_pymorphy2Lemmatize = train_model(train_data, with_report=True, Lemmatizer=pymorphy2Lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.945\n"
     ]
    }
   ],
   "source": [
    "model_MystemLemmatize = train_model(train_data, with_report=True, Lemmatizer=MystemLemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.939\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "model_WordNetLemmatize = train_model(train_data, with_report=True, Lemmatizer=WordNetLemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на тестовой выборке: 0.971\n",
      "F1-score на валидационной выборке: 0.939\n"
     ]
    }
   ],
   "source": [
    "model_without_lemmatize = train_model(train_data, with_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "train_data = pd.read_csv('train.csv')\n",
    "train_data['title'] = pymorphy2Lemmatize(train_data['title'])\n",
    "train_data = clean_data(train_data)\n",
    "train_data['text'] = train_data['title'].apply(lambda x: x + ' ') + train_data['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['title'].apply(lambda x: x + ' ') + 4 * train_data['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Преобразование текста в числа\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(train_data['text'])\n",
    "y = train_data['target']\n",
    "\n",
    "# Разбиение данных на обучающую и валидационную выборки\n",
    "X_train = X\n",
    "y_train = y\n",
    "# Обучение модели\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SGDClassifier(class_weight=\"balanced\")\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df['title'] = pymorphy2Lemmatize(test_df['title'])\n",
    "test_df = clean_data(test_df)\n",
    "test_df['text'] = test_df['title'].apply(lambda x: x + ' ') + test_df['url']\n",
    "X_test_vectorized = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['title'].apply(lambda x: x + ' ') + 4 * test_df['url']\n",
    "X_test_vectorized = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165378, 178426)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    145014\n",
      "True      20364\n",
      "Name: target, dtype: int64\n",
      "id,target\r\n",
      "135309,False\r\n",
      "135310,False\r\n",
      "135311,False\r\n",
      "135312,True\r\n",
      "135313,False\r\n",
      "135314,False\r\n",
      "135315,False\r\n",
      "135316,False\r\n",
      "135317,False\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "test_df[\"target\"] = model2.predict(X_test_vectorized).astype(bool)\n",
    "\n",
    "counts = test_df[\"target\"].value_counts()\n",
    "print(counts)\n",
    "\n",
    "test_df[[\"id\", \"target\"]].to_csv(\"ml_baseline_SGDClass_cov.csv\", index=False)\n",
    "\n",
    "!cat ml_baseline.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(\"ml_baseline_outClean_merged.csv\")[\"target\"].value_counts()\n",
    "print(counts)\n",
    "counts = pd.read_csv(\"ml_baseline_CleanLemmatize_Merged_CoV.csv\")[\"target\"].value_counts()\n",
    "print(counts)\n",
    "counts = pd.read_csv(\"ml_baseline_CleanLemmatize_Merged_CoV_NoSplit.csv\")[\"target\"].value_counts()\n",
    "print(counts)\n",
    "print(\"BEST:\")\n",
    "counts = pd.read_csv(\"ml_baseline_CleanLemmatizeStem_Merged_CoV_NoSplit.csv\")[\"target\"].value_counts()\n",
    "print(counts)\n",
    "counts = pd.read_csv(\"ml_baseline.csv\")[\"target\"].value_counts()\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
