{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве домашнего задания вам предлагается поработать над предсказанием погоды. Файл с данными вы найдете в соответствующей директории. Вам будет доступен датасет weather.csv, ПЕРВЫЕ 75% (shuffle = False) которого нужно взять для обучения, последние 25% - для тестирования.\n",
    "\n",
    "Требуется построить 4 модели которые будут предсказывать целевую переменную <b>RainTomorrow</b> с помощью:\n",
    "\n",
    "   1. логистической регрессии [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "   \n",
    "   2. метода ближайших соседей [sklearn.neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    " \n",
    "   3. Байесовского классификатора [sklearn.naive_bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "   \n",
    "   4. логистической регрессии реализованной самостоятельно\n",
    "\n",
    "Затем следует сравнить результаты моделей (по качеству и времени выполнения) и сделать вывод о том, какая модель и с какими параметрами даёт лучшие результаты.\n",
    "\n",
    "Не забывайте о том, что работа с признаками играет очень большую роль в построении хорошей модели.\n",
    "\n",
    "Краткое описание данных:\n",
    "\n",
    "    Date - Дата наблюдений\n",
    "    Location - Название локации, в которой расположена метеорологическая станция\n",
    "    MinTemp - Минимальная температура в градусах цельсия\n",
    "    MaxTemp - Максимальная температура в градусах цельсия\n",
    "    Rainfall - Количество осадков, зафиксированных за день в мм\n",
    "    Evaporation - Так называемое \"pan evaporation\" класса А (мм) за 24 часа до 9 утра\n",
    "    Sunshine - Число солнечных часов за день\n",
    "    WindGustDir - направление самого сильного порыва ветра за последние 24 часа\n",
    "    WindGustSpeed - скорость (км / ч) самого сильного порыва ветра за последние 24 часа\n",
    "    WindDir9am - направление ветра в 9 утра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающий и тестовый наборы\n",
    "train_data, test_data = train_test_split(data, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на признаки и целевую переменную\n",
    "X_train = train_data.drop('RainTomorrow', axis=1)\n",
    "y_train = train_data['RainTomorrow']\n",
    "\n",
    "X_test = test_data.drop('RainTomorrow', axis=1)\n",
    "y_test = test_data['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование категориальных признаков\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Определение индексов колонок с категориальными признаками\n",
    "categorical_features = ['Date']\n",
    "\n",
    "# Создание объекта ColumnTransformer для кодирования категориальных признаков\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Преобразование данных\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вручную подсчитываю некоторую статистику\n",
    "def manual_report(y_test, y_pred):\n",
    "    # Количество неверно предсказанных\n",
    "    error_count = np.count_nonzero(y_test != y_pred)\n",
    "    # Количество верно предсказанных\n",
    "    success_count = np.count_nonzero(y_test == y_pred)\n",
    "\n",
    "    # True Positive\n",
    "    TP = np.sum((y_pred == 'Yes') & (y_test == 'Yes'))\n",
    "    # True Negative\n",
    "    TN = np.sum((y_pred == 'No') & (y_test == 'No'))\n",
    "\n",
    "    # False Positive\n",
    "    FP = np.sum((y_pred == 'Yes') & (y_test == 'No'))\n",
    "    # False Negative\n",
    "    FN = np.sum((y_pred == 'No') & (y_test == 'Yes'))\n",
    "    \n",
    "    # Вывод\n",
    "    df_Negative = pd.DataFrame({'True Negative': [TN], 'False Negative': [FN], 'Precision': [TN/(TN+FN)],\n",
    "                               'Precision by total count': [success_count/y_pred.shape[0]]}, index=[\"No\"])\n",
    "    df_Positive = pd.DataFrame({'True Positive': [TP], 'False Positive': [FP], 'Precision': [TP/(TP+FP)], \n",
    "                                'Precision by total count': [error_count/y_pred.shape[0]]}, index=[\"Yes\"])\n",
    "\n",
    "    print(df_Negative)\n",
    "    print()\n",
    "    print(df_Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def train_model(X_train, y_train, X_test, y_test, model, manual=False):\n",
    "    # Обучение модели на обучающем наборе данных\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Предсказание целевой переменной для тестового набора данных\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print('\\n\\t==============', type(model).__name__, '==============')\n",
    "    if manual:\n",
    "        manual_report(y_test, y_pred)\n",
    "    print('\\n\\t-------------- Classification report --------------')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Обучим модель методом LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t============== LogisticRegression ==============\n",
      "    True Negative  False Negative  Precision  Precision by total count\n",
      "No          26180            7135   0.785832                  0.751414\n",
      "\n",
      "     True Positive  False Positive  Precision  Precision by total count\n",
      "Yes            532            1702   0.238138                  0.248586\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.94      0.86     27882\n",
      "         Yes       0.24      0.07      0.11      7667\n",
      "\n",
      "    accuracy                           0.75     35549\n",
      "   macro avg       0.51      0.50      0.48     35549\n",
      "weighted avg       0.67      0.75      0.69     35549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Создание объекта модели\n",
    "model = LogisticRegression()\n",
    "\n",
    "train_model(X_train, y_train, X_test, y_test, model, manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Обучим модель методом ближайших соседей (neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t============== KNeighborsClassifier ==============\n",
      "    True Negative  False Negative  Precision  Precision by total count\n",
      "No          22434            5855   0.793029                  0.682045\n",
      "\n",
      "     True Positive  False Positive  Precision  Precision by total count\n",
      "Yes           1812            5448   0.249587                  0.317955\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.80      0.80     27882\n",
      "         Yes       0.25      0.24      0.24      7667\n",
      "\n",
      "    accuracy                           0.68     35549\n",
      "   macro avg       0.52      0.52      0.52     35549\n",
      "weighted avg       0.68      0.68      0.68     35549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors # Почему я вообще могу вызвать метод fit для такого метода, \n",
    "# что там тренировать, если всегда высчитывается расстояние заново для каждого элемента. Нужен только predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Создание объекта модели\n",
    "# model = NearestNeighbors(n_neighbors=2, algorithm='ball_tree') # Выдает ошибку отутствия метода predict - странно\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "train_model(X_train, y_train, X_test, y_test, model, manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Обучим модель методом Байесовского классификатора (naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t============== MultinomialNB ==============\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.93      0.85     27882\n",
      "         Yes       0.25      0.08      0.12      7667\n",
      "\n",
      "    accuracy                           0.75     35549\n",
      "   macro avg       0.52      0.51      0.49     35549\n",
      "weighted avg       0.67      0.75      0.70     35549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "train_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Отчеты разных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t============== LogisticRegression ==============\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.94      0.86     27882\n",
      "         Yes       0.24      0.07      0.11      7667\n",
      "\n",
      "    accuracy                           0.75     35549\n",
      "   macro avg       0.51      0.50      0.48     35549\n",
      "weighted avg       0.67      0.75      0.69     35549\n",
      "\n",
      "\n",
      "\t============== KNeighborsClassifier ==============\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.80      0.80     27882\n",
      "         Yes       0.25      0.24      0.24      7667\n",
      "\n",
      "    accuracy                           0.68     35549\n",
      "   macro avg       0.52      0.52      0.52     35549\n",
      "weighted avg       0.68      0.68      0.68     35549\n",
      "\n",
      "\n",
      "\t============== MultinomialNB ==============\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.93      0.85     27882\n",
      "         Yes       0.25      0.08      0.12      7667\n",
      "\n",
      "    accuracy                           0.75     35549\n",
      "   macro avg       0.52      0.51      0.49     35549\n",
      "weighted avg       0.67      0.75      0.70     35549\n",
      "\n",
      "\n",
      "\t============== BernoulliNB ==============\n",
      "\n",
      "\t-------------- Classification report --------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.93      0.85     27882\n",
      "         Yes       0.25      0.08      0.12      7667\n",
      "\n",
      "    accuracy                           0.75     35549\n",
      "   macro avg       0.52      0.51      0.49     35549\n",
      "weighted avg       0.67      0.75      0.70     35549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "models = [LogisticRegression(), \n",
    "          KNeighborsClassifier(n_neighbors=1), \n",
    "          MultinomialNB(), \n",
    "          BernoulliNB(),\n",
    "         ]\n",
    "\n",
    "for model in models:\n",
    "    train_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Обучим модель методом логистической регрессии реализованной самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ниже не мой код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация логистической регрессии\n",
    "__Логистическая регрессия__\n",
    "\n",
    "$$p(y|x) = a(x, \\theta) = \\sigma(\\langle x, \\theta \\rangle) = \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "weights = np.array([1, 2, 3])\n",
    "\n",
    "X =  np.array([[ 1,  1, 1],\n",
    "               [-1, -2, 1],\n",
    "               [-1, -2, 2],\n",
    "               [-2, -2, -3]\n",
    "              ])\n",
    "\n",
    "y = np.array([1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0] [9.97527377e-01 1.19202922e-01 7.31058579e-01 3.05902227e-07]\n"
     ]
    }
   ],
   "source": [
    "def probability(weights, X):\n",
    "    z = X @ weights # вычисляем линейную комбинацию\n",
    "    result = 1 / (1 + np.exp(-z)) # применяем сигмоидную функцию\n",
    "    return result\n",
    "\n",
    "prob = probability(weights, X)\n",
    "\n",
    "labels = np.round(prob).astype(int) # преобразуем вероятности в метки классов\n",
    "print(labels, prob)\n",
    "\n",
    "assert type(prob) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert prob.shape == (X.shape[0],), 'Неверный размер массива'\n",
    "assert (prob.round(3) == [0.998, 0.119, 0.731, 0.]).all(), 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция предсказания метки класса, получает на вход вероятности принадлежности к классу 1 и выдает метки классов $y \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_class_prediction(weights, X, threshold =.5):\n",
    "    prob =  probability(weights, X)\n",
    "    labels = np.round(prob).astype(int) # преобразуем вероятности в метки классов\n",
    "    return labels\n",
    "\n",
    "y_pred = binary_class_prediction(weights, X)\n",
    "\n",
    "\n",
    "assert type(y_pred) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert y_pred.shape == (X.shape[0],), 'Неверный размер массива'\n",
    "assert min(y_pred) == 0, 'Функция считается неверно'\n",
    "assert max(y_pred) == 1, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Функционал качества логистической регрессии__\n",
    "\n",
    "Запишем правдободовие выборки для меток класса $y \\in \\{+1, -1\\}$ \n",
    "\n",
    "$$Likelihood(a, X^\\ell) = \\prod_{i = 1}^{\\ell} a(x_i,\\theta)^{[y_i = +1]} (1 - a(x_i, \\theta))^{[y_i = -1]} → \\operatorname*{max}_{\\theta}$$ \n",
    "\n",
    "Прологарифмируем правдоподобие выборки и перейдем к задаче минимизации:\n",
    "\n",
    "$$Q(a, X^\\ell) =     -\\sum_{i = 1}^{\\ell} \n",
    "        [y_i = +1] \\log a(x_i, \\theta)\n",
    "        +\n",
    "        [y_i = -1] \\log (1 - a(x_i, \\theta)) \\to \\operatorname*{min}_{\\theta}$$ \n",
    "        \n",
    "Подставим $a(x, \\theta)$ в функцинал качества:\n",
    "\n",
    "$$ Q(a, X^\\ell) = -\\sum_{i = 1}^{\\ell} \\left(\n",
    "    [y_i = +1]\n",
    "    \\log \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "    +\n",
    "    [y_i = -1]\n",
    "    \\log \\frac{\\exp(-\\langle \\theta, x_i \\rangle)}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "\\right)\n",
    "=\\\\\n",
    "=\n",
    "-\\sum_{i = 1}^{\\ell} \\left(\n",
    "    [y_i = +1]\n",
    "    \\log \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "    +\n",
    "    [y_i = -1]\n",
    "    \\log \\frac{1}{1 + \\exp(\\langle \\theta, x_i \\rangle)}\n",
    "\\right)\n",
    "=\\\\\n",
    "=\n",
    "\\sum_{i = 1}^{\\ell}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(-y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый оптимизируемый функционал качества (logloss), записанный для меток классов $y \\in \\{+1, -1\\}$ и усредненный по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a, X^\\ell) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(-y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right) \\to \\operatorname*{min}_{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем его в функции logloss:\n",
    "#### ! Сказано неверно. У нас классы 0 и 1, для них вероятность определяем по другому"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(weights, X, y): \n",
    "    # YOUR CODE HERE\n",
    "    prob =  probability(weights, X)\n",
    "    \n",
    "#     print(np.mean(np.log(1+np.exp(-y))))\n",
    "#     print(np.mean(np.log(1+np.exp(-y * (X @ weights)))))\n",
    "#     print(np.mean(np.log(1+np.exp(-y * prob))))\n",
    "#     print(np.mean(-y * np.log(1 / (1 + np.exp(-X @ weights)) - (1-y) * np.log(1 - 1 / (1 + np.exp(-X @ weights))) )))\n",
    "#     print(np.mean(-y * np.log(prob) - (1-y) * np.log(1 - prob) ))\n",
    "    result = np.mean(-y * np.log(prob) - (1-y) * np.log(1 - prob) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logloss(weights, X, y).round(3) == 0.861, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Алгоритм оптимизации функционала качества. Стохастический градиентный спуск__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вход: </b> Выборка $X^\\ell$, темп обучения $h$\n",
    "\n",
    "<b>Выход: </b> оптимальный вектор весов $\\theta$\n",
    "\n",
    "1.  Инициализировать веса $\\theta$\n",
    "2.  Инициализировать оценку функционала качества: $Q(a, X^\\ell)$\n",
    "3.  <b>Повторять</b>: \n",
    "\n",
    "    Выбрать случайным образом подвыборку объектов $X^{batch} =\\{x_1, \\dots,x_n \\}$ из $X^{\\ell}$\n",
    "    \n",
    "    Рассчитать градиент функционала качества: $\\nabla Q(X^{batch}, \\theta)$\n",
    "    \n",
    "    Обновить веса: $\\theta := \\theta - h\\cdot \\nabla Q(X^{batch}, \\theta)$\n",
    "       \n",
    "    <b>Пока</b> значение $Q$ и/или веса $\\theta$ не сойдутся   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию рассчета градиента функционала качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial Q(a, X^{batch}) }{\\partial \\theta_j}   = \\frac{\\partial \\frac{1}{n}\\sum_{i = 1}^{n}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(- y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right)} {\\partial \\theta_j}  = \\frac{1}{n}\\sum_{i = 1}^{n}\n",
    "     \\frac {1}{\n",
    "        1 + \\exp(- y_i \\langle \\theta, x_i \\rangle)} \\cdot  \\exp(- y_i \\langle \\theta, x_i \\rangle) \\cdot -y_i x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте рассчет градиента в матричном виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result \n",
    "\n",
    "assert gradient(theta, X, y).shape == theta.shape, 'Неверный размер массива'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обучения уже реализована"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, batch_size=10, h=0.05,  iters=100, plot=True):\n",
    "\n",
    "    # получаем размерности матрицы\n",
    "    size, dim = X.shape\n",
    "\n",
    "    # случайная начальная инициализация\n",
    "    theta = np.random.uniform(size=dim)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    theta_history = theta\n",
    "    colors = [plt.get_cmap('gist_rainbow')(i) for i in np.linspace(0,1,dim)]\n",
    "    \n",
    "    # plt \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax3 = fig.add_subplot(212)\n",
    "        fig.suptitle('Gradient descent')\n",
    "        \n",
    "        \n",
    "    for _ in range(iters):  \n",
    "        \n",
    "        # берём случайный набор элементов\n",
    "        batch = np.random.choice(size, batch_size, replace=False)\n",
    "        X_batch = X[batch]\n",
    "        y_batch = y[batch]\n",
    "\n",
    "        # считаем производные\n",
    "        grad = gradient(theta, X_batch, y_batch)\n",
    "        \n",
    "        assert type(grad) == np.ndarray, 'неверный тип'\n",
    "        assert len(grad.shape) == 1, 'Необходимо вернуть одномерный вектор'\n",
    "        assert grad.shape[0] == len(theta), 'длина вектора должна быть равной количеству весов'\n",
    "        \n",
    "        \n",
    "        # Обновляем веса\n",
    "        \n",
    "        theta -= grad * h\n",
    "        \n",
    "        theta_history = np.vstack((theta_history, theta))\n",
    "        \n",
    "        # error\n",
    "        loss = logloss(theta, X, y)\n",
    "        errors.append(loss)\n",
    "        \n",
    "        if plot:\n",
    "            ax1.clear()            \n",
    "            ax1.scatter(range(dim), theta, label='Gradient solution')\n",
    "            ax1.legend(loc=\"upper left\")\n",
    "            ax1.set_title('theta')\n",
    "            ax1.set_ylabel(r'$\\bar \\beta$')\n",
    "            ax1.set_xlabel('weight ID')\n",
    "            \n",
    "            \n",
    "            ax2.plot(range(_+1), errors, 'g-')\n",
    "            ax2.set_title('logloss')\n",
    "            ax2.set_xlabel('itarations')\n",
    "            \n",
    "            ax3.plot(theta_history)\n",
    "            ax3.set_title('update theta')\n",
    "            ax3.set_ylabel('value')\n",
    "            ax3.set_xlabel('itarations')\n",
    "            time.sleep(0.05)\n",
    "            fig.canvas.draw()   \n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_theta = fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_class_prediction(optimal_theta, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
